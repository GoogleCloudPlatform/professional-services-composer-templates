# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before configuring YAML and generate DAG, please setup Inspection and De-Identification templates which can be used for the job

# To configure follow the steps mentioned below
# 1. Go to Google Cloud Console --> search for sensitive-data-protection

#Creating Inspection templates
# https://cloud.google.com/sensitive-data-protection/docs/creating-templates-inspect

#creating Deidentification templates
# https://cloud.google.com/sensitive-data-protection/docs/creating-templates-deid#create_a_de-identification_template

# Few open source examples
# https://cloud.google.com/sensitive-data-protection/docs/samples

# Helpful API resources for further configurations
# https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dlp/index.html#airflow.providers.google.cloud.operators.dlp.CloudDLPCreateDLPJobOperator 
# https://cloud.google.com/sensitive-data-protection/docs/reference/rest/v2/InspectJobConfig#CloudStorageOptions 
# https://cloud.google.com/sensitive-data-protection/docs/reference/rest/v2/projects.deidentifyTemplates#DeidentifyTemplate.DeidentifyConfig
# https://cloud.google.com/python/docs/reference/dlp/latest/google.cloud.dlp_v2.types.InspectJobConfig 

# Logical flow and othe examples can be
# Once process finishes writing to GCS, we can trigger DLP job to de-identify and write results to another GCS location for further consumption

# To Export data to GCS using this DAG tmplate please configure the Service account with apporiate roles and access as mentioned:
# https://cloud.google.com/sql/docs/mysql/import-export/import-export-csv#export_data_from

# DAG parameters
# mandatory
dag_id: data_loss_preventive_deidentify_tasks_job     
description:
max_active_runs:
catchup: False
schedule_interval: "None"        
tags: ["test"]
owner:


task_variables:
  variables_file_path: examples/composer_dag_tasks_variables/data_loss_preventive_tasks_variables.py


# Define Python functions to be added in your Airflow DAG
custom_python_functions:
  custom_defined_functions:


# Tasks specific configs
# mandatory
tasks:
  - task_id: trigger_dlp_deindentify_job
    task_type: airflow.providers.google.cloud.operators.dlp.CloudDLPCreateDLPJobOperator
    project_id: gcp_project_id
    inspect_job: dlp_job_payload
    trigger_rule : 'all_done'
    depends_on: 


task_dependency:
  default_task_dependency: True

