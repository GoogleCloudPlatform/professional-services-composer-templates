# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# DAG parameters
# mandatory
dag_id: dataproc_tasks_dag
description:
max_active_runs:
catchup: False
schedule_interval: None
tags: ["test"]
owner:


# Optional. List imports required outside of task operators
additional_imports:


task_variables:
  variables_file_path: examples/composer_dag_tasks_variables/dataproc_tasks_variables.py


# Define Python functions to be added in your Airflow DAG
custom_python_functions:
  custom_defined_functions:


# Tasks specific configs
# mandatory
tasks:
  - task_id: create_cluster
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator
    cluster_name: cluster_name
    region: region
    project_id: project_id
    cluster_config: create_cluster_config
    depends_on: 
    trigger_rule : 'none_failed'
  - task_id: update_cluster
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocUpdateClusterOperator
    cluster_name: cluster_name
    project_id: project_id
    region: region
    graceful_decommission_timeout: {"seconds": 600}
    cluster: update_cluster_config
    update_mask: {"paths":["config.worker_config.num_instances","config.secondary_worker_config.num_instances"]}
    deferrable: True
    depends_on: 
      - create_cluster
    trigger_rule : 'none_failed'
  - task_id: hadoop_job
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocSubmitJobOperator
    region: region
    project_id: project_id
    job: hadoop_job_config
    deferrable: True
    depends_on: 
      - update_cluster
    trigger_rule : 'none_failed'
  - task_id: spark_job
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocSubmitJobOperator
    region: region
    project_id: project_id
    job: spark_job_config
    deferrable: True
    depends_on: 
      - update_cluster
    trigger_rule : 'none_failed'
  - task_id: delete_cluster
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator
    cluster_name: cluster_name
    region: region
    project_id: project_id
    depends_on: 
      - spark_job
    trigger_rule : 'none_failed'
  - task_id: create_template
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocCreateWorkflowTemplateOperator
    template: create_template_config
    project_id: project_id
    region: region
    depends_on: 
      - delete_cluster
    trigger_rule : 'all_done'
  - task_id: start_template_job
    task_type: airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator
    template_id: "sparkpi"
    project_id: project_id
    region: region
    depends_on: 
      - create_template
    trigger_rule : 'none_failed'


task_dependency:
  default_task_dependency: True

