# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before configuring YAML and generate DAG, please read regarding Airflow Cloud spanner limited support operators as mentioned :
# https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/spanner/index.html 

# SpannerQueryDatabaseInstanceOperator: This only support DML operations.
# For this example we will be using PythonOperator to import/export data from/to Google Cloud Storage after running a Delete statement 
# on the table as an example.

# Sample Cloud Spanner DDL as per the example
# CREATE TABLE Products (
#     ProductId INT64 NOT NULL,
#     ProductName STRING(MAX) NOT NULL,
#     Description STRING(MAX),
#     Price NUMERIC,
#     LastModified TIMESTAMP
# ) PRIMARY KEY(ProductId);

# Sample data as in CSV format uploaded to GCS
# 1,Laptop,High-performance laptop,1200,2024-11-01T11:01:22.796411616Z
# 2,Mouse,Wireless mouse,25,2024-11-02T17:01:22.796411616Z
# 3,Keyboard,Ergonomic keyboard,75,2024-11-05T08:01:22.796411616Z
# 4,Cell Phone,Google Pixel 9 pro,950.89,2024-11-06T18:01:22.796411616Z

# To Export data to GCS using this DAG tmplate please configure the Service account with apporiate roles and access as mentioned:
# https://cloud.google.com/sql/docs/mysql/import-export/import-export-csv#export_data_from



# DAG parameters
# mandatory
dag_id: cloudspanner_import_export_tasks_dag     
description: "Sample example to import/export data in and out of cloud spanner."
max_active_runs:
catchup: False
# mandatory; enclose values within quotes
schedule_interval: "None"        
tags: ["spanner","test","v1.1"]
owner:

# mandatory
default_args:
    owner: 'test'
    retries:
    email_on_failure: False
    email_on_retry: False
    # ex - [test@test.com]
    email:                          
    # minutes
    retry_delay: 1                  

#Python functions used by Python operator or any other custom operator.
# If there are no function keep it empty
# Logical flow:
  # 1. Define values for import_functions_from_file (Acceptable values are True or False)
  # 2. If import_functions_from_file is True, copy the full file path for reading python \
  #   function (ONLY supports local file path)
  # 3. If import_functions_from_file is False, add your custom function in the same YAML \
  #   as shown in the example. The DAG will be generated accordingly
custom_python_functions:
  import_functions_from_file: False
  functions_file_path: /Users/devanshmodi/vscode/professional-services-composer-templates/examples/python_function_files/cloudspanneroperator_functions.txt
  custom_defined_functions:
    upload_gcs_to_spanner:
      description: Uploads data from a CSV file in GCS to a Cloud Spanner table.
      code: |
        def upload_gcs_to_spanner(
          project_id:str, instance_id:str, database_id:str, bucket_name:str, file_name:str, table_name:str,columns:list):
          """Uploads data from a CSV file in GCS to a Cloud Spanner table."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to read files
          :param str file_name: Filename to import to 
          :param str table_name: Cloud Spanner Table name for importing data
          :param list columns: Cloud Spanner table column names in ascending order as per DDL (assumed that input file is in same order)
          :return dict : GCS file path and Spanner details
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Download the file from GCS
          data = blob.download_as_string()

          # Parse the CSV data
          rows = []
          for line in data.decode("utf-8").splitlines():
              row = line.split(",")
              rows.append(row)

          # Insert data into Cloud Spanner table
          cols = columns
          with database.batch() as batch:
              batch.insert(table=table_name, columns=cols, values=rows)

          print(f"Data from {file_name} uploaded to {table_name} successfully.")
          return {"input_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}", "spanner_databse_table":f"{database_id}"+":"+f"{table_name}"}
    export_spanner_to_gcs:
      description: Exports data from Cloud Spanner to a CSV file in GCS.
      code: |
        def export_spanner_to_gcs(
          project_id, instance_id, database_id, bucket_name, file_name, sql_query):
          """Exports data from Cloud Spanner to a CSV file in GCS."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to export files to
          :param str sql_query: Spanner SQL query to fetch data from spanner 
          :return dict : GCS output file path
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Execute the query and fetch data
          with database.snapshot() as snapshot:
              results = snapshot.execute_sql(sql_query)

          # Format data as CSV
          csv_data = ""
          for row in results:
              csv_data += ",".join(str(value) for value in row) + "\n"

          # Upload the CSV data to GCS
          blob.upload_from_string(csv_data)

          print(f"Data exported to {file_name} in GCS successfully.")
          return {"output_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}"}
    # pull_xcom:
    #   description: Function to pull xcom variables from export GCS task print or use it for other transformations.
    #   code: |
    #     def pull_xcom(**kwargs):
    #       """
    #       Pulls a value from XCom and prints it.
    #       """
    #       ti = kwargs['ti']
    #       pulled_value = str(ti.xcom_pull(task_ids='export_sales_reporting_table_to_gcs', key='file_details'))
    #       print(f"Pulled value from XCom: {pulled_value}")
    #       return pulled_value

#Tasks specific configs
# mandatory
envs:
    # mandatory
    default:
        # DAG tasks configs
        # mandatory
        tasks:
        #cc_operator_description: Ingest data from GCS to Cloud Spanner
        - task_id: gcs_to_spanner
          task_type: airflow.operators.python_operator.PythonOperator
          python_callable: upload_gcs_to_spanner
          op_kwargs: {"project_id":cc_var_gcp_project_id,"instance_id":cc_var_spanner_instance_id,"database_id":cc_var_spanner_databse_id,"bucket_name":cc_var_import_gcs_bucket_name,"file_name":cc_var_import_gcs_file_name,"table_name":cc_var_spanner_table,"columns":cc_var_spanner_table_columns}
          trigger_rule : 'all_done'
          depends_on: None
        #cc_operator_description: Delete old data from Cloud spanner table (an example to showcase SpannerQueryDatabaseInstanceOperator)
        - task_id: delete_results_from_spanner
          task_type: airflow.providers.google.cloud.operators.spanner.SpannerQueryDatabaseInstanceOperator
          instance_id: cc_var_spanner_instance_id
          database_id: cc_var_spanner_databse_id
          query: cc_var_spanner_sql_query 
          project_id: cc_var_gcp_project_id
          trigger_rule : 'all_done'
          depends_on: gcs_to_spanner
        #cc_operator_description: Export data from Cloud Spanner to GCS
        - task_id: spanner_to_gcs
          task_type: airflow.operators.python_operator.PythonOperator
          python_callable: export_spanner_to_gcs
          op_kwargs: {"project_id":cc_var_gcp_project_id,"instance_id":cc_var_spanner_instance_id,"database_id":cc_var_spanner_databse_id,"bucket_name":cc_var_export_gcs_bucket_name,"file_name":cc_var_export_gcs_file_name,"sql_query":cc_var_spanner_sql_export_query}
          trigger_rule : 'all_done'
          depends_on: delete_results_from_spanner
        #cc_operator_description: print the GCS uri where the file is written.
        # - task_id: print_gcs_uri
        #   task_type: airflow.operators.python_operator.PythonOperator
        #   python_callable: pull_xcom
        #   trigger_rule : 'all_done'
        #   depends_on: export_sales_reporting_table_to_gcs
    task_dependency:
    # Define task_dependency as per your use case
    # Logical flow:
      # 1. Define values for default_task_dependency (Acceptable values are True or False).\
          # If default_task_dependency is not set appropriately it will raise error exception.
      # 2. If default_task_dependency is True, it will work add task as per depends_on configuration \
          # set under each task
      # 3. If default_task_dependency is False, add your custom task dependency the code will validate as per\
          # given tasks and will add the dependency while generating the DAG.
      # 4. Seperate your custom task dependency using "|" as a seperator and other dependencies in new \
          # line as shown in the example below 
      default_task_dependency: False
      custom_task_dependency: "start >> gcs_to_spanner >> delete_results_from_spanner >> spanner_to_gcs|\
                              [start >> gcs_to_spanner] >> spanner_to_gcs|\
                              start >> [delete_results_from_spanner,spanner_to_gcs]"
    task_variables:
    # Define task_variables used by your tasks
    # If you are reading variables from .yaml file. \
    # PLEASE UPLOAD FILES TO gs://<YOUR_COMPOSER_ENV_NAME>/dag_variables/<YOUR_FILE_NAME>.yaml
    # Logical flow:
      # 1. Define values for import_variables_from_file (Acceptable values are True or False).\
          # Else it won't import variables appropriately
      # 2. If import_variables_from_file is True, it will read the YAML and \
          # will read variables from GCS during airflow execution
      # 3. Define values for environment as per your YAML and it will \
          # read variables from GCS according to environment mentioned in YAML.
      # 4. If import_variables_from_file is False the variables will be imported \
          # as deined under config_variables
      import_variables_from_file: True
      variables_file_name: composer_variables_test.yaml
      environment: dev
      config_variables:
        cc_var_gcp_project_id: composer-templates-dev
        cc_var_spanner_instance_id: composer-templates-spannerdb
        cc_var_spanner_databse_id: dev-spannerdb
        cc_var_spanner_table: Products
        cc_var_spanner_table_columns: ["ProductId","ProductName","Description","Price","LastModified"]
        cc_var_spanner_sql_query: "DELETE FROM Products WHERE LastModified < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 4 DAY);"
        cc_var_import_gcs_bucket_name: composer-templates-dev-input-files
        cc_var_import_gcs_file_name: spanner_input/sample_data_spanner.csv
        cc_var_export_gcs_bucket_name: hmh_backup
        cc_var_export_gcs_file_name: spanner_output/product_data_output.csv
        cc_var_spanner_sql_export_query: "SELECT * FROM Products ;"
