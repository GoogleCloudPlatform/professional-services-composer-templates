# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before configuring YAML and generate DAG, please read regarding Airflow Cloud spanner limited support operators as mentioned :
# https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/spanner/index.html 

# SpannerQueryDatabaseInstanceOperator: This only support DML operations.
# For this example we will be using PythonOperator to import/export data from/to Google Cloud Storage after running a Delete statement 
# on the table as an example.

# Sample Cloud Spanner DDL as per the example
# CREATE TABLE Products (
#     ProductId INT64 NOT NULL,
#     ProductName STRING(MAX) NOT NULL,
#     Description STRING(MAX),
#     Price NUMERIC,
#     LastModified TIMESTAMP
# ) PRIMARY KEY(ProductId);

# Sample data as in CSV format uploaded to GCS
# 1,Laptop,High-performance laptop,1200,2024-11-01T11:01:22.796411616Z
# 2,Mouse,Wireless mouse,25,2024-11-02T17:01:22.796411616Z
# 3,Keyboard,Ergonomic keyboard,75,2024-11-05T08:01:22.796411616Z
# 4,Cell Phone,Google Pixel 9 pro,950.89,2024-11-06T18:01:22.796411616Z

# To Export data to GCS using this DAG tmplate please configure the Service account with apporiate roles and access as mentioned:
# https://cloud.google.com/sql/docs/mysql/import-export/import-export-csv#export_data_from



# DAG parameters
# mandatory
dag_id: cloudspanner_import_export_tasks_dag     
description: "Sample example to import/export data in and out of cloud spanner."
max_active_runs:
catchup: False
schedule: 
tags: ["spanner","test","v1.1"]
start_date:
end_date:
max_active_tasks: 5
dagrun_timeout: timedelta(hours=3)
is_paused_upon_creation: True

# mandatory
default_args:
  owner: 'test'
  depends_on_past: False
  retries: 3
  email_on_failure: False
  email_on_retry: False
  email: ["test@example.com"]                       
  retry_delay: 1  # minutes   
  mode: "reschedule"
  poke_interval: 120
  sla: 60            

# Define Python functions to be added in your Airflow DAG
# - import_functions_from_file:  
#   - True:  Load functions from a local Python file (specify the 'file_path').
#   - False: Define functions directly within this YAML configuration.
# - functions: In-place code.
custom_python_functions:
  import_functions_from_file: False
  functions_file_path: examples/python_function_files/cloudspanneroperator_python_functions.py
  custom_defined_functions:
    upload_gcs_to_spanner:
      description: Uploads data from a CSV file in GCS to a Cloud Spanner table.
      code: |
        def upload_gcs_to_spanner(
          project_id:str, instance_id:str, database_id:str, bucket_name:str, file_name:str, table_name:str,columns:list):
          """Uploads data from a CSV file in GCS to a Cloud Spanner table."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to read files
          :param str file_name: Filename to import to 
          :param str table_name: Cloud Spanner Table name for importing data
          :param list columns: Cloud Spanner table column names in ascending order as per DDL (assumed that input file is in same order)
          :return dict : GCS file path and Spanner details
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Download the file from GCS
          data = blob.download_as_string()

          # Parse the CSV data
          rows = []
          for line in data.decode("utf-8").splitlines():
              row = line.split(",")
              rows.append(row)

          # Insert data into Cloud Spanner table
          cols = columns
          with database.batch() as batch:
              batch.insert(table=table_name, columns=cols, values=rows)

          print(f"Data from {file_name} uploaded to {table_name} successfully.")
          return {"input_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}", "spanner_databse_table":f"{database_id}"+":"+f"{table_name}"}
    export_spanner_to_gcs:
      description: Exports data from Cloud Spanner to a CSV file in GCS.
      code: |
        def export_spanner_to_gcs(
          project_id, instance_id, database_id, bucket_name, file_name, sql_query):
          """Exports data from Cloud Spanner to a CSV file in GCS."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to export files to
          :param str sql_query: Spanner SQL query to fetch data from spanner 
          :return dict : GCS output file path
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Execute the query and fetch data
          with database.snapshot() as snapshot:
              results = snapshot.execute_sql(sql_query)

          # Format data as CSV
          csv_data = ""
          for row in results:
              csv_data += ",".join(str(value) for value in row) + "\n"

          # Upload the CSV data to GCS
          blob.upload_from_string(csv_data)

          print(f"Data exported to {file_name} in GCS successfully.")
          return {"output_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}"}
    print_args:
      code: |
        def print_args(*args):
          """Prints all arguments passed to the function.

          Args:
            *args: Any number of arguments.
          """
          for arg in args:
            print(arg)


# Tasks specific configs
# mandatory
tasks:
  # cc_operator_description: Ingest data from GCS to Cloud Spanner
  - task_id: gcs_to_spanner
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: upload_gcs_to_spanner
    op_kwargs: {"project_id":cc_var_gcp_project_id,"instance_id":cc_var_spanner_instance_id,"database_id":cc_var_spanner_databse_id,"bucket_name":cc_var_import_gcs_bucket_name,"file_name":cc_var_import_gcs_file_name,"table_name":cc_var_spanner_table,"columns":cc_var_spanner_table_columns}
    trigger_rule : 'all_done'
    depends_on: None
  # cc_operator_description: Delete old data from Cloud spanner table (an example to showcase SpannerQueryDatabaseInstanceOperator)
  - task_id: delete_results_from_spanner
    task_type: airflow.providers.google.cloud.operators.spanner.SpannerQueryDatabaseInstanceOperator
    instance_id: cc_var_spanner_instance_id
    database_id: cc_var_spanner_databse_id
    query: cc_var_spanner_sql_query 
    project_id: cc_var_gcp_project_id
    trigger_rule : 'all_done'
    depends_on: gcs_to_spanner
  # cc_operator_description: Export data from Cloud Spanner to GCS
  - task_id: spanner_to_gcs
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: export_spanner_to_gcs
    op_kwargs: {"project_id":cc_var_gcp_project_id,"instance_id":cc_var_spanner_instance_id,"database_id":cc_var_spanner_databse_id,"bucket_name":cc_var_export_gcs_bucket_name,"file_name":cc_var_export_gcs_file_name,"sql_query":cc_var_spanner_sql_export_query}
    trigger_rule : 'all_done'
    depends_on: delete_results_from_spanner
  - task_id: sample_python
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: print_args
    op_args: ['Composer', 'template', cc_var_gcp_project_id]
    depends_on: spanner_to_gcs


task_dependency:
  # Controls task dependency management.
  # - default_task_dependency: Controls default dependency behavior.
  #   - If True, tasks will depend on each other based on the `depends_on` configuration within each task definition.
  #   - If False, use `custom_task_dependency` to define dependencies.
  # - custom_task_dependency:  Explicitly define dependencies
  
  # Examples (Each entry `-` defines a separate dependency chain in the generated Airflow DAG. Must be enclosed with double-quotes):
  #   - "task_a >> task_b >> task_c"  
  #   - "[task_a, task_b] >> task_c" 
  #   - "task_a >> task_b | task_c >> task_d" 
  default_task_dependency: False
  custom_task_dependency: 
    - "start >> gcs_to_spanner >> delete_results_from_spanner >> spanner_to_gcs"
    - "[start >> gcs_to_spanner] >> spanner_to_gcs"
    - "start >> [delete_results_from_spanner, spanner_to_gcs]"
    - spanner_to_gcs >> sample_python


task_variables:
  # Load variables from a YAML file in GCS. The file should be at:
  # gs://<YOUR_COMPOSER_ENV_NAME>/dag_variables/<variables_file_name>
  import_from_file: True  # Boolean to enable/disable file import
  file_name: cloudspanner_tasks_variables.yaml
  environment: dev  # Environment to load from the file

  # Define variables here if not importing from file.
  variables:
    cc_var_gcp_project_id: composer-templates-dev
    cc_var_spanner_instance_id: composer-templates-spannerdb
    cc_var_spanner_databse_id: dev-spannerdb
    cc_var_spanner_table: Products
    cc_var_spanner_table_columns: ["ProductId","ProductName","Description","Price","LastModified"]
    cc_var_spanner_sql_query: "DELETE FROM Products WHERE LastModified < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 4 DAY);"
    cc_var_import_gcs_bucket_name: composer-templates-dev-input-files
    cc_var_import_gcs_file_name: spanner_input/sample_data_spanner.csv
    cc_var_export_gcs_bucket_name: hmh_backup
    cc_var_export_gcs_file_name: spanner_output/product_data_output.csv
    cc_var_spanner_sql_export_query: "SELECT * FROM Products ;"
