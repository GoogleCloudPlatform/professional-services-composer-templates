# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Before configuring YAML and generate DAG, please read regarding Airflow Cloud spanner limited support operators as mentioned :
# https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/spanner/index.html 

# SpannerQueryDatabaseInstanceOperator: This only support DML operations.
# For this example we will be using PythonOperator to import/export data from/to Google Cloud Storage after running a Delete statement 
# on the table as an example.

# Sample Cloud Spanner DDL as per the example
# CREATE TABLE Products (
#     ProductId INT64 NOT NULL,
#     ProductName STRING(MAX) NOT NULL,
#     Description STRING(MAX),
#     Price NUMERIC,
#     LastModified TIMESTAMP
# ) PRIMARY KEY(ProductId);

# Sample data as in CSV format uploaded to GCS
# 1,Laptop,High-performance laptop,1200,2024-11-01T11:01:22.796411616Z
# 2,Mouse,Wireless mouse,25,2024-11-02T17:01:22.796411616Z
# 3,Keyboard,Ergonomic keyboard,75,2024-11-05T08:01:22.796411616Z
# 4,Cell Phone,Google Pixel 9 pro,950.89,2024-11-06T18:01:22.796411616Z

# To Export data to GCS using this DAG tmplate please configure the Service account with apporiate roles and access as mentioned:
# https://cloud.google.com/sql/docs/mysql/import-export/import-export-csv#export_data_from



# DAG parameters
# mandatory
dag_id: cloudspanner_import_export_tasks_dag     
description: "Sample example to import/export data in and out of cloud spanner."
max_active_runs:
catchup: False
schedule: None
tags: ["spanner","test","v1.1"]
start_date:
end_date:
max_active_tasks: 5
dagrun_timeout: timedelta(hours=3)
is_paused_upon_creation: True


# Optional. List imports required outside of task operators
additional_imports:
  - module: from google.cloud import spanner, storage


task_variables:
  variables_file_path: examples/composer_dag_tasks_variables/cloudspanner_tasks_variables.py


# Define Python functions to be added in your Airflow DAG
# - import_functions_from_file:  
#   - True:  Load functions from a local Python file (specify the 'file_path').
#   - False: Define functions directly within this YAML configuration.
# - functions: In-place code.
custom_python_functions:
  import_functions_from_file: False
  functions_file_path: examples/python_function_files/cloudspanneroperator_python_functions.py
  custom_defined_functions:
    upload_gcs_to_spanner:
      description: Uploads data from a CSV file in GCS to a Cloud Spanner table.
      code: |
        def upload_gcs_to_spanner(
          project_id:str, instance_id:str, database_id:str, bucket_name:str, file_name:str, table_name:str,columns:list):
          """Uploads data from a CSV file in GCS to a Cloud Spanner table."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to read files
          :param str file_name: Filename to import to 
          :param str table_name: Cloud Spanner Table name for importing data
          :param list columns: Cloud Spanner table column names in ascending order as per DDL (assumed that input file is in same order)
          :return dict : GCS file path and Spanner details
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Download the file from GCS
          data = blob.download_as_string()

          # Parse the CSV data
          rows = []
          for line in data.decode("utf-8").splitlines():
              row = line.split(",")
              rows.append(row)

          # Insert data into Cloud Spanner table
          cols = columns
          with database.batch() as batch:
              batch.insert(table=table_name, columns=cols, values=rows)

          print(f"Data from {file_name} uploaded to {table_name} successfully.")
          return {"input_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}", "spanner_databse_table":f"{database_id}"+":"+f"{table_name}"}
    export_spanner_to_gcs:
      description: Exports data from Cloud Spanner to a CSV file in GCS.
      code: |
        def export_spanner_to_gcs(
          project_id, instance_id, database_id, bucket_name, file_name, sql_query):
          """Exports data from Cloud Spanner to a CSV file in GCS."""
          """
          :param str project_id: Google Cloud Project ID
          :param str instance_id: Google Cloud Spanner Instance ID
          :param str database_id: Google Cloud Spanner Database ID
          :param str bucket_name: Google Cloud Storage Bucket to export files to
          :param str sql_query: Spanner SQL query to fetch data from spanner 
          :return dict : GCS output file path
          """

          # Initialize Cloud Spanner client
          spanner_client = spanner.Client(project=project_id)
          instance = spanner_client.instance(instance_id)
          database = instance.database(database_id)

          # Initialize Cloud Storage client
          storage_client = storage.Client(project=project_id)
          bucket = storage_client.bucket(bucket_name)
          blob = bucket.blob(file_name)

          # Execute the query and fetch data
          with database.snapshot() as snapshot:
              results = snapshot.execute_sql(sql_query)

          # Format data as CSV
          csv_data = ""
          for row in results:
              csv_data += ",".join(str(value) for value in row) + "\n"

          # Upload the CSV data to GCS
          blob.upload_from_string(csv_data)

          print(f"Data exported to {file_name} in GCS successfully.")
          return {"output_file_path":"gs://"+f"{bucket_name}"+"/"+f"{file_name}"}
    print_args:
      code: |
        def print_args(*args):
          """Prints all arguments passed to the function.

          Args:
            *args: Any number of arguments.
          """
          for arg in args:
            print(arg)


# Tasks specific configs
# mandatory
tasks:
  - task_id: gcs_to_spanner
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: upload_gcs_to_spanner
    op_kwargs: |
      {
        "project_id": gcp_project_id,
        "instance_id": spanner_instance_id,
        "database_id": spanner_databse_id,
        "bucket_name": import_gcs_bucket_name,
        "file_name": import_gcs_file_name,
        "table_name": spanner_table,
        "columns": spanner_table_columns
      }
    trigger_rule : 'all_done'
    depends_on: 
      - extract_data
  - task_id: delete_results_from_spanner
    task_type: airflow.providers.google.cloud.operators.spanner.SpannerQueryDatabaseInstanceOperator
    instance_id: spanner_instance_id
    database_id: spanner_databse_id
    query: spanner_sql_query 
    project_id: gcp_project_id
    trigger_rule : 'all_done'
    depends_on: 
      - gcs_to_spanner
  - task_id: spanner_to_gcs
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: export_spanner_to_gcs
    op_kwargs: |
      {"project_id":gcp_project_id,"instance_id":spanner_instance_id,"database_id":spanner_databse_id,"bucket_name":export_gcs_bucket_name,"file_name":export_gcs_file_name,"sql_query":spanner_sql_export_query}
    trigger_rule : 'all_done'
    depends_on: 
      - delete_results_from_spanner
  - task_id: sample_python
    task_type: airflow.operators.python_operator.PythonOperator
    python_callable: print_args
    op_args: |
      ['Composer', 'template', gcp_project_id]
    depends_on: 
      - delete_results_from_spanner


task_groups:
  - group_id: my_task_group1
    depends_on: 
    tasks:
      - task_id: bash_example1
        task_type: airflow.operators.bash.BashOperator
        bash_command: |
          echo "bash_example1"
          date
          ls -l
      - task_id: bash_example2
        task_type: airflow.operators.bash.BashOperator
        bash_command: |
          echo "bash_example2"
          pwd
  - group_id: extract_data
    depends_on: 
      - my_task_group1
    tasks:
      - task_id: extract_from_source_a
        task_type: airflow.providers.postgres.operators.postgres.PostgresOperator
        sql: SELECT * FROM source_a;
        depends_on:
          - bash_example1
      - task_id: extract_from_source_b
        task_type: airflow.providers.postgres.operators.postgres.PostgresOperator
        sql: SELECT * FROM source_b;
        depends_on:
          - bash_example2


task_dependency:
  # Controls task dependency management.
  # - default_task_dependency: Controls default dependency behavior.
  #   - If True, tasks will depend on each other based on the `depends_on` configuration within each task definition.
  #   - If False, use `custom_task_dependency` to define dependencies.
  # - custom_task_dependency:  Explicitly define dependencies
  
  # Examples (Each entry `-` defines a separate dependency chain in the generated Airflow DAG. Must be enclosed with double-quotes):
  #   - "task_a >> task_b >> task_c"  
  #   - "[task_a, task_b] >> task_c" 
  #   - "task_a >> task_b | task_c >> task_d" 
  default_task_dependency: True
  custom_task_dependency: 
    - "my_task_group1 >> extract_data"
    - "bash_example1 >> extract_from_source_a"
    - "bash_example2 >> extract_from_source_b"
    - "extract_data >> gcs_to_spanner >> delete_results_from_spanner >> [spanner_to_gcs, sample_python]"


