# Copyright 2024 Google LLC

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     https://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# DAG parameters
# mandatory
dag_id: bigquery_tasks_dag     
description:
max_active_runs:
catchup: False
schedule_interval: '@hourly'        
tags: ["test"]
owner:


# Optional. List imports required outside of task operators
additional_imports:


# Define variables and DAG default_args in [.py] file
task_variables:
  variables_file_path: examples/composer_dag_tasks_variables/bigquery_tasks_variables.py


# Define Python functions to be added in your Airflow DAG
# - import_functions_from_file:  
#   - True:  Load functions from a local Python file (specify the 'file_path').
#   - False: Define functions directly within this YAML configuration.
# - functions: In-place code.
custom_python_functions:
  custom_defined_functions:
    perform_transformation:
      description: Sample function as an example to perform custom transformation.
      code: |
        def transformation(data):
          """
          Sample function as an example to perform custom transformation.

          Args:
            data: Sample data on which we can perform any transformation.
          
          Returns:
            The data converted into a string format.
          """
          print("Printing sample payload from transformation function: {}".format(data))
          output = str(data)
          return output
    pull_xcom:
      description: Function to pull xcom variables from export GCS task print or use it for other transformations.
      code: |
        def pull_xcom(**kwargs):
          """
          Pulls a value from XCom and prints it.
          """
          ti = kwargs['ti']
          pulled_value = str(ti.xcom_pull(task_ids='export_sales_reporting_table_to_gcs', key='file_details'))
          print(f"Pulled value from XCom: {pulled_value}")
          return pulled_value



# Tasks specific configs
# mandatory
tasks:
  #cc_operator_description: Create a new dataset for your Project in BigQuery.
  - task_id: create_bq_dataset
    task_type: airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator
    dataset_id: 'test_dataset'
    project_id: project_id
    depends_on: 
    trigger_rule : 'none_failed'
  #cc_operator_description: Creates a new table in the specified BigQuery dataset, optionally with schema.
  - task_id: create_bq_table
    task_type: airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyTableOperator
    dataset_id: 'test_dataset'
    table_id: 'test_table'
    schema_fields: [{"name": "emp_name", "type": "STRING", "mode": "REQUIRED"},{"name": "salary", "type": "INTEGER", "mode": "NULLABLE"}]
    depends_on: 
      - create_bq_dataset
    trigger_rule : 'none_failed'
  #cc_operator_description: Fetch data and return it, either from a BigQuery table, or results of a query job.
  - task_id: get_data_from_bq_table
    task_type: airflow.providers.google.cloud.operators.bigquery.BigQueryGetDataOperator
    dataset_id: 'test_dataset'
    table_id: 'test_table'
    depends_on: 
      - create_bq_dataset
    trigger_rule : 'none_failed'
  #cc_operator_description: Transfers a BigQuery table to a Google Cloud Storage bucket.
  - task_id: export_to_gcs
    task_type: airflow.providers.google.cloud.transfers.bigquery_to_gcs.BigQueryToGCSOperator
    source_project_dataset_table: export_to_gcs_source_project_dataset_table
    destination_cloud_storage_uris: export_to_gcs_destination_cloud_storage_uris
    export_format: csv
    field_delimiter: ','
    print_header: True
    depends_on: 
      - get_data_from_bq_table
    trigger_rule : 'none_failed'
  #cc_operator_description: Executes BigQuery SQL queries in a specific BigQuery database.
  - task_id: bq_query_execute
    task_type: airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator
    use_legacy_sql: False
    write_disposition: WRITE_TRUNCATE
    allow_large_results: True
    destination_dataset_table: destination_dataset_table
    sql: sql
    depends_on: 
      - export_to_gcs
    trigger_rule : 'none_failed'


task_dependency:
  default_task_dependency: True

